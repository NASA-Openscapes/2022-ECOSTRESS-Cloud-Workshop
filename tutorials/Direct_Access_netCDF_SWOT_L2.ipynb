{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "In this notebook will will show direct access of archive products in the AWS Simple Storage Service (S3). In this demo, we will showcase the usage of **SWOT Simulated Level-2 KaRIn SSH from GLORYS for Science Version 1**. More information on the datasets can be found at https://podaac.jpl.nasa.gov/dataset/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1.\n",
    "\n",
    "We will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n",
    "\n",
    "In the future, if you want to use this notebook as a reference, please note that we are not doing collection discovery here- we assume the collection of interest has been determined. \n",
    "\n",
    "### Requirements\n",
    "AWS\n",
    "This notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\n",
    "\n",
    "The notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\n",
    "\n",
    "Most of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n",
    "\n",
    "```\n",
    "boto3\n",
    "s3fs\n",
    "xarray\n",
    "matplotlib\n",
    "cartopy\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "* import needed libraries\n",
    "* authenticate for NASA Earthdata archive (Earthdata Login)\n",
    "* obtain AWS credentials for Earthdata DAAC archive in AWS S3\n",
    "* access DAAC data by downloading directly from S3 within US-west 2 and operating on those files.\n",
    "* access DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\n",
    "* plot the first time step in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import pyplot as plt\n",
    "from os import path\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a temporary AWS Access Key based on your Earthdata Login user ID\n",
    "\n",
    "By accessing https://archive.podaac.earthdata.nasa.gov/s3credentials, you will be given an AWS Access credential. This key will last 1 hour and will give you access to PO.DAAC S3 Collection buckets. We will store this key in our environment variables for use by the btot3 s3fs libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the result of your accessing and login to the s3Credential endpoint above into the 's3_credential' variable here:\n",
    "# https://archive.podaac.earthdata.nasa.gov/s3credentials\n",
    "s3_credential ='''\n",
    "<<PASTE RESULT OF https://archive.podaac.earthdata.nasa.gov/s3credentials HERE>>\n",
    "'''\n",
    "creds = json.loads(s3_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = creds[\"accessKeyId\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = creds[\"secretAccessKey\"]\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = creds[\"sessionToken\"]\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location of data in the PO.DAAC S3 Archive\n",
    "We need to determine the path for our products of interest. We can do this through several mechanisms.\n",
    "\n",
    "## Finding S3 Location information from the PO.DAAC Portal\n",
    "The easiest of which is through the PO.DAAC Cloud Dataset Listing page: https://podaac.jpl.nasa.gov/cloud-datasets\n",
    "\n",
    "![S3 Data Locations from Portal](img/S3_data_locations_from_portal.png)\n",
    "\n",
    "For eachd ataset, the 'Data Access' tab will have various information, but will always contain the S3 paths listed specifically. Data files will *always* be found under the 'protected' bucket.\n",
    "\n",
    "## Finding S3 Location from Earthdata Search\n",
    "\n",
    "From the Earthdata Search Client (search.earthdata.nasa.gov), collection level information can be found by clicking the 'i' on a collection search result. An example of this is seen below:\n",
    "\n",
    "![S3 Data Locations from Search 1](img/S3_data_locations_from_search_1.png)\n",
    "\n",
    "\n",
    "Once on the collection inforamtion screen, the S3 bucket locations can be found by scrolling to the bottom of the information panel. The SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1 example is shown below.\n",
    "\n",
    "![S3 Data Locations from Search 2](img/S3_data_locations_from_search_2.png)\n",
    "\n",
    "\n",
    "\n",
    "## Finding S3 Location from CMR\n",
    "\n",
    "One can query the collection identifier to get information from CMR:\n",
    "\n",
    "```\n",
    "https://cmr.earthdata.nasa.gov/search/concepts/C2152045877-POCLOUD.umm_json\n",
    "```\n",
    "\n",
    "The identifier is found on the PO.DAAC [Cloud Data Set Listing](https://podaac.jpl.nasa.gov/cloud-datasets) page entries, called 'Collection Concept ID'\n",
    "\n",
    "Results returned will look like the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "    ...\n",
    "    \"DirectDistributionInformation\": {\n",
    "        \"Region\": \"us-west-2\",\n",
    "        \"S3BucketAndObjectPrefixNames\": [\n",
    "            \"podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\",\n",
    "            \"podaac-ops-cumulus-public/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/\"\n",
    "        ],\n",
    "        \"S3CredentialsAPIEndpoint\": \"https://archive.podaac.earthdata.nasa.gov/s3credentials\",\n",
    "        \"S3CredentialsAPIDocumentationURL\": \"https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME\"\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that we have the bucket location...\n",
    "\n",
    "It's time to find our data! Below we are using a 'glob' to find file names matching a pattern. Here, we want any files matching the pattern used below, this equates, in science, terms, to Cycle 001 and the first 10 passes. This information can be gleaned form product description documents. Another way of finding specific data files would be to search on cycle/pass from CMR and use the S3 links provided in the resulting metadata directly instead of doing a glob (essentially an 'ls')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 's3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_00*.nc'\n",
    "remote_files = s3.glob(s3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Access - get these files from S3 and store them on your running instance\n",
    "\n",
    "Here we will leverage the speed of transfering data within the cloud to our running instance (this notebook!). We will download 10 files into the 'DEMO_FILES' directory to show you cloud and traditional access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for f in remote_files:\n",
    "    s3.download(f, \"DEMO_FILES/\" + os.path.basename(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_mfdataset(\"DEMO_FILES/*.nc\", combine='nested', concat_dim=\"num_lines\", decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ssh_karin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot these 10 files in a chosen projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 12))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "ds.ssh_karin.plot.pcolormesh(\n",
    " ax=ax, transform=ccrs.PlateCarree(), x=\"longitude\", y=\"latitude\", add_colorbar=False\n",
    ")\n",
    "ax.coastlines()\n",
    "\n",
    "#ds.ssh_karin.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Files without any Downloads to your disk\n",
    "\n",
    "We can also do that same plot without 'downloading' the data to our disk first. Let's try access the data from S3 directly through xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 's3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_00*.nc'\n",
    "remote_files = s3.glob(s3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = [s3.open(file) for file in remote_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "data = xr.open_mfdataset(fileset,engine='h5netcdf', combine='nested', concat_dim=\"num_lines\", decode_times=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ssha_karin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 12))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "data.ssha_karin.plot.pcolormesh(\n",
    " ax=ax, transform=ccrs.PlateCarree(), x=\"longitude\", y=\"latitude\", add_colorbar=False\n",
    ")\n",
    "ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A final word...\n",
    "\n",
    "Accessing data completely from S3 and in memory are affected by various things.\n",
    "\n",
    "1. The format of the data- archive formats like NetCDF, GEOTIFF, HDF  vs cloud optimized data structures (Zarr, kerchunk, cog). cloud formats are made for accessing only the pieces of data of interest needed at the time of request (e.g. a subset, timestep, etc)\n",
    "2. The internal structure of the data. Tools like xarray make a lot of assumptions about how to open and read a file. Sometimes the internals don't fit the xarray 'mold' and we need to continue to work with data providers and software providers to make these two sides work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
