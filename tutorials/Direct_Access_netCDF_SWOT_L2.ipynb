{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "In this notebook, we will access monthly sea surface height from ECCO V4r4 (10.5067/ECG5D-SSH44). The data are provided as a time series of monthly netCDFs on a 0.5-degree latitude/longitude grid.\n",
    "\n",
    "We will access the data from inside the AWS cloud (us-west-2 region, specifically) and load a time series made of multiple netCDF datasets into a single xarray dataset. This approach leverages S3 native protocols for efficient access to the data.\n",
    "\n",
    "### Requirements\n",
    "AWS\n",
    "This notebook should be running in an EC2 instance in AWS region us-west-2, as previously mentioned. We recommend using an EC2 with at least 8GB of memory available.\n",
    "\n",
    "The notebook was developed and tested using a t2.small instance (_ CPUs; 8GB memory). Python 3\n",
    "\n",
    "Most of these imports are from the Python standard library. However, you will need to install these packages into your Python 3 environment if you have not already done so:\n",
    "\n",
    "```\n",
    "s3fs\n",
    "xarray\n",
    "matplotlib\n",
    "cartopy\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "* import needed libraries\n",
    "* define dataset of interest\n",
    "* authenticate for NASA Earthdata archive (Earthdata Login)\n",
    "* obtain AWS credentials for Earthdata DAAC archive in AWS S3\n",
    "* access DAAC data directly from the in-region S3 bucket without moving or downloading any files to your local (cloud) workspace\n",
    "* plot the first time step in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import xarray as xr\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste the result of your accessing and login to the s3Credential endpoint above into the 's3_credential' variable here:\n",
    "s3_credential = ''\n",
    "creds = json.loads(s3_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start simple with a LocalCluster that makes use of all the cores and RAM we have on a single machine\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "# explicitly connect to the cluster we just created\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = creds[\"accessKeyId\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = creds[\"secretAccessKey\"]\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = creds[\"sessionToken\"]\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False) \n",
    "\n",
    "s3path = 's3://podaac-ops-cumulus-protected/SWOT_SIMULATED_L2_KARIN_SSH_GLORYS_SCIENCE_V1/SWOT_L2_LR_SSH_Expert_001_00*.nc'\n",
    "#s3path = 's3://podaac-ops-cumulus-protected/ECCO_L4_ATM_STATE_05DEG_DAILY_V4R4/ATM_SURFACE_TEMP_HUM_WIND_PRES_day_mean_1992-01-*.nc'\n",
    "remote_files = s3.glob(s3path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = [s3.open(file) for file in remote_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "data = xr.open_mfdataset(fileset,engine='h5netcdf', combine='nested', concat_dim=\"num_lines\", decode_times=False)\n",
    "# OR\n",
    "# datasets = []\n",
    "# for f in fileset:\n",
    "#     print(\"Openning \" + str(f) )\n",
    "#     f = xr.open_dataset(f, engine='h5netcdf', decode_times=False)\n",
    "#     datasets.append(f)\n",
    "# xr.concat(datasets, dim=\"num_lines\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.ssha_karin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "from matplotlib import pyplot as plt\n",
    "import hvplot.xarray  # noqa\n",
    "\n",
    "plt.figure(figsize=(21, 12))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "data.ssha_karin.plot.pcolormesh(\n",
    " ax=ax, transform=ccrs.PlateCarree(), x=\"longitude\", y=\"latitude\", add_colorbar=False\n",
    ")\n",
    "ax.coastlines()\n",
    "#ax.set_ylim([0, 180]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
